{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tfc\n",
    "import os\n",
    "from load_data import *\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' #使用 GPU \n",
    "def use_gpu_polite(using_rate=0.6):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = using_rate\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickle data from ../data/imdb_data_3col.pkl\n",
      "Original 134957 words in vocabulary.\n",
      "After truncated low frequent word:\n",
      "words num: 40000/134957; words freq: 0.981\n",
      "Words exit in w2v file: 39210/40004, rate: 98.015198%\n",
      "Shape of weight matrix: (40006, 50)\n",
      "Train data shape: (25000, 500) label length: 25000\n",
      "Test data shape: (25000, 500) label length: 25000\n",
      "dict_keys(['data', 'data_len', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# 导入数据，train data 是一个字典， ebd embdding 是词向量矩阵，作为embedding层的初始参数\n",
    "train_data, test_data, ebd_weights = load_imdb_data()\n",
    "print(train_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text bilstm 模型，使用layers接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextBiLstm():\n",
    "    def __init__(self, class_num, ebd_weights, lstm_units=128):\n",
    "        self.seq_input = tf.placeholder(dtype=tf.int32, shape=[None, None],\n",
    "                                        name='sequence_input')\n",
    "        self.seq_length = tf.placeholder(dtype=tf.int32, shape=[None], name='seq_length')\n",
    "        self.sparse_label_input = tf.placeholder(dtype=tf.int32, shape=[None],\n",
    "                                                 name='sparse_label')\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.global_step_op = tf.assign(self.global_step, self.global_step+1)\n",
    "        # 使用动态指数递减学习率\n",
    "        self.learning_rate = tf.train.exponential_decay(0.0015, self.global_step, decay_steps=10, \n",
    "                                       decay_rate=0.9, staircase=True)\n",
    "        \n",
    "        embedding_dim = ebd_weights.shape[1]\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.W = tf.Variable(initial_value=ebd_weights, name='W')\n",
    "            # batch * seq_len * emb_dim\n",
    "            self.embedding_layer = tf.nn.embedding_lookup(self.W, self.seq_input)\n",
    "        \n",
    "        # 正反 双向的 LSTM cell，加入了dropout\n",
    "        fw_cell = tfc.rnn.DropoutWrapper(tfc.rnn.LSTMCell(num_units=lstm_units), output_keep_prob=0.8)\n",
    "        bw_cell = tfc.rnn.DropoutWrapper(tfc.rnn.LSTMCell(num_units=lstm_units), output_keep_prob=0.8)\n",
    "        \n",
    "        outputs, status = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, self.embedding_layer,\n",
    "                                                           sequence_length=self.seq_length, dtype=tf.float32)\n",
    "        # 将输出的 序列 和 最终状态 正反向拼接\n",
    "        self.outputs_concat = tf.concat(outputs, axis=-1)\n",
    "        (f_c, f_h), (b_c, b_h) = status\n",
    "        self.status_concat = tf.concat([f_h, b_h], axis=-1)\n",
    "        \n",
    "        with tf.name_scope('output'):\n",
    "            self.logits = tf.layers.dense(self.status_concat, class_num)\n",
    "            self.prediction = tf.argmax(self.logits, axis=-1, output_type=tf.int32)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.sparse_label_input,\n",
    "                                                                    logits=self.logits)\n",
    "            self.loss_sum = tf.reduce_sum(losses)\n",
    "            self.loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(self.prediction, self.sparse_label_input)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_prediction, tf.float16))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float16))\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        gs_vs = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(gs_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dutir923/wujiaming/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /home/dutir923/wujiaming/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "Epoch 10, train loss 0.6926, acc 0.4984; test loss 0.6929, acc 0.4986. lr: 0.001350\n",
      "Epoch 20, train loss 0.6921, acc 0.4987; test loss 0.6929, acc 0.4985. lr: 0.001215\n",
      "Epoch 30, train loss 0.6904, acc 0.4985; test loss 0.6929, acc 0.4995. lr: 0.001093\n",
      "Epoch 40, train loss 0.6859, acc 0.5121; test loss 0.6916, acc 0.5062. lr: 0.000984\n",
      "Epoch 50, train loss 0.6447, acc 0.5545; test loss 0.6822, acc 0.5338. lr: 0.000886\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = train_data['data'], [1 if i == 'pos' else 0 for i in train_data['label']]\n",
    "test_x, test_y = test_data['data'], [1 if i == 'pos' else 0 for i in test_data['label']]\n",
    "train_x_len, test_x_len = train_data['data_len'], test_data['data_len']\n",
    "lstm_model = TextBiLstm(class_num=2, ebd_weights=ebd_weights, lstm_units=128)\n",
    "batch_size = 2000\n",
    "epoch_max = 50\n",
    "\n",
    "config = use_gpu_polite(0.8)\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch_now = sess.run(lstm_model.global_step)\n",
    "    \n",
    "    while epoch_now < epoch_max:\n",
    "        # 在训练集上按batch训练完所有，算作一个epoch\n",
    "        batch_num = train_x.shape[0] // batch_size\n",
    "        for i in range(batch_num+1):\n",
    "            s_i = i * batch_size\n",
    "            e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "            if s_i >= e_i:\n",
    "                continue\n",
    "            in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "            in_x_len = train_x_len[s_i: e_i]\n",
    "            feed_dict = {lstm_model.seq_input: in_x, lstm_model.sparse_label_input: in_y,\n",
    "                        lstm_model.seq_length: in_x_len}\n",
    "            sess.run(lstm_model.train_op, feed_dict)\n",
    "        epoch_now = sess.run(lstm_model.global_step_op)\n",
    "        \n",
    "        if epoch_now % 10 == 0:  # 每10轮观察一下训练集测试集loss 和 acc\n",
    "            # 训练集总的损失和acc也要分步测，否则内存不够\n",
    "            batch_num = train_x.shape[0] // batch_size\n",
    "            train_total_loss = 0\n",
    "            train_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "                in_x_len = train_x_len[s_i: e_i]\n",
    "                feed_dict = {lstm_model.seq_input: in_x, lstm_model.sparse_label_input: in_y,\n",
    "                            lstm_model.seq_length: in_x_len}\n",
    "\n",
    "                train_loss_one, train_correct_one = sess.run([lstm_model.loss_sum, lstm_model.correct_num], feed_dict)\n",
    "                train_total_loss += train_loss_one\n",
    "                train_total_correct += train_correct_one\n",
    "            train_loss = train_total_loss / train_x.shape[0]\n",
    "            train_acc = train_total_correct / train_x.shape[0]\n",
    "\n",
    "            # 测试集的损失和acc\n",
    "            batch_num = test_x.shape[0] // batch_size\n",
    "            test_total_loss = 0\n",
    "            test_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, test_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = test_x[s_i: e_i, :], test_y[s_i: e_i]\n",
    "                in_x_len = test_x_len[s_i: e_i]\n",
    "                feed_dict = {lstm_model.seq_input: in_x, lstm_model.sparse_label_input: in_y,\n",
    "                            lstm_model.seq_length: in_x_len}\n",
    "\n",
    "                test_loss_one, test_correct_one = sess.run([lstm_model.loss_sum, lstm_model.correct_num], feed_dict)\n",
    "                test_total_loss += test_loss_one\n",
    "                test_total_correct += test_correct_one\n",
    "            test_loss = test_total_loss / test_x.shape[0]\n",
    "            test_acc = test_total_correct / test_x.shape[0]\n",
    "            \n",
    "            lr_now = sess.run(lstm_model.learning_rate)\n",
    "            print('Epoch %d, train loss %.4f, acc %.4f; test loss %.4f, acc %.4f. lr: %f' % \n",
    "                  (epoch_now, train_loss, train_acc, test_loss, test_acc, lr_now))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型用了一个很简单的双向LSTM，维度大小128  \n",
    "## 训练了50轮但还是只有55%左右，可见拟合得十分慢  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
