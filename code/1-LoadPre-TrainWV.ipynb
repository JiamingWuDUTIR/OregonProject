{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 使用TextProcessor类处理文本，使用gensim导入已有的预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from text_preprocess import TextProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 5 words in vocabulary.\n",
      "After truncated low frequent word:\n",
      "words num: 5/5; words freq: 1.000\n",
      "length of sentence: length : freq\n",
      "2 1\n",
      "3 1\n",
      "5 1\n",
      "\n",
      "单词表前四个是预设定的占位符\n",
      "vocabulary: ['<pad>', '<s>', '<\\\\s>', '<unk>', 'a', 'b', 'c', 'd', 'e']\n"
     ]
    }
   ],
   "source": [
    "sent_lst = ['a b c d e',\n",
    "           'a d e',\n",
    "           'c b']\n",
    "preprocessor = TextProcessor(sent_lst)\n",
    "preprocessor.build_word_freq_dct()\n",
    "preprocessor.build_word2id()   # 构造单词表，可以输入最大词数做截断，否则全部都要\n",
    "preprocessor.view_sent_length_freq()  # 观察句长统计信息\n",
    "print('\\n单词表前四个是预设定的占位符')\n",
    "print('vocabulary:', preprocessor.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 这是一个w2v格式的词向量文件，首行表示词数量，维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3\n",
      "a 0.418 0.24968 -0.41242\n",
      "b 0.013441 0.23682 -0.16899\n",
      "c 0.70853 0.57088 -0.4716\n",
      "d 0.68047 -0.039263 0.30186\n",
      "e 0.26818 0.14346 -0.27877\n"
     ]
    }
   ],
   "source": [
    "with open('../data/1-test_w2v.txt', 'r', encoding='utf-8') as file_open:\n",
    "    print(file_open.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words exit in w2v file: 5/9, rate: 55.555556%\n",
      "Shape of weight matrix: (11, 3)\n",
      "\n",
      "1留给未登录词，从2开始对词表中单词建立词向量\n",
      "word | id | embedding\n",
      "<pad> 2 [ 0.08079735  0.08735236 -0.08746586]\n",
      "<s> 3 [-0.00064265 -0.0525261   0.01152341]\n",
      "<\\s> 4 [-0.07846112  0.01590114 -0.02880822]\n",
      "<unk> 5 [ 0.06748111  0.02766662 -0.09496042]\n",
      "a 6 [ 0.418    0.24968 -0.41242]\n",
      "b 7 [ 0.013441  0.23682  -0.16899 ]\n",
      "c 8 [ 0.70853  0.57088 -0.4716 ]\n",
      "d 9 [ 0.68047  -0.039263  0.30186 ]\n",
      "e 10 [ 0.26818  0.14346 -0.27877]\n"
     ]
    }
   ],
   "source": [
    "key_weights = KeyedVectors.load_word2vec_format('../data/1-test_w2v.txt')   # 载入这个文件，得到word2vec的一种类\n",
    "\n",
    "preprocessor.build_weights(keyed_vectors=key_weights)\n",
    "\n",
    "print('\\n1留给未登录词，从2开始对词表中单词建立词向量')\n",
    "print('word | id | embedding')\n",
    "for word in preprocessor.vocab:\n",
    "    word_id = preprocessor.word2id[word]\n",
    "    print(word, word_id, preprocessor.weights[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将句子序列padding成长度为7的id序列\n",
      "a b c d e\n",
      "[ 6  7  8  9 10  0  0] 5\n",
      "a d e\n",
      "[ 6  9 10  0  0  0  0] 3\n",
      "c b\n",
      "[8 7 0 0 0 0 0] 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "a c e bbb a as\n",
      "[ 6  8 10  1  6  1  0] 6\n",
      "a b de ab df as d bb c ad\n",
      "[6 7 1 1 1 1 9] 7\n",
      "ab bc cd de\n",
      "[1 1 1 1 0 0 0] 4\n"
     ]
    }
   ],
   "source": [
    "print('将句子序列padding成长度为7的id序列')\n",
    "\n",
    "id_array, sent_len_arr = preprocessor.get_truncate_id_list(sent_lst, truncated_len=7)  # 获得这组句子的id表示，同时获得原始长度\n",
    "for i in range(len(sent_lst)):\n",
    "    print(sent_lst[i])\n",
    "    print(id_array[i, :], sent_len_arr[i])\n",
    "\n",
    "print('-'*100)\n",
    "test_sent_lst = ['a c e bbb a as',\n",
    "                 'a b de ab df as d bb c ad',\n",
    "                 'ab bc cd de']\n",
    "test_id_array, test_sent_len_arr = preprocessor.get_truncate_id_list(test_sent_lst, truncated_len=7)  # 在一组测试句子上做实验\n",
    "for i in range(len(test_sent_lst)):\n",
    "    print(test_sent_lst[i])\n",
    "    print(test_id_array[i, :], test_sent_len_arr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 观察keras中Embedding层是怎么运行的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子列表：\n",
      "['a b c d e', 'a d e', 'c b']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "词向量列表：\n",
      "[[[ 0.418     0.24968  -0.41242 ]\n",
      "  [ 0.013441  0.23682  -0.16899 ]\n",
      "  [ 0.70853   0.57088  -0.4716  ]\n",
      "  [ 0.68047  -0.039263  0.30186 ]\n",
      "  [ 0.26818   0.14346  -0.27877 ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]]\n",
      "\n",
      " [[ 0.418     0.24968  -0.41242 ]\n",
      "  [ 0.68047  -0.039263  0.30186 ]\n",
      "  [ 0.26818   0.14346  -0.27877 ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]]\n",
      "\n",
      " [[ 0.70853   0.57088  -0.4716  ]\n",
      "  [ 0.013441  0.23682  -0.16899 ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# Embedding 层定义：字典长度；词向量维度；是不是对0进行mask；初始化参数，注意加'[]'\n",
    "model.add(Embedding(input_dim=len(preprocessor.weights), output_dim=3, mask_zero=True, weights=[preprocessor.weights]))\n",
    "embedding_result = model.predict(id_array)\n",
    "print('句子列表：')\n",
    "print(sent_lst)\n",
    "print('-'*100)\n",
    "print('词向量列表：')\n",
    "print(embedding_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集句子列表：\n",
      "['a c e bbb a as', 'a b de ab df as d bb c ad', 'ab bc cd de']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "测试集词向量列表：\n",
      "[[[ 0.418       0.24968    -0.41242   ]\n",
      "  [ 0.70853     0.57088    -0.4716    ]\n",
      "  [ 0.26818     0.14346    -0.27877   ]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.418       0.24968    -0.41242   ]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.418       0.24968    -0.41242   ]\n",
      "  [ 0.013441    0.23682    -0.16899   ]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.68047    -0.039263    0.30186   ]]\n",
      "\n",
      " [[ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.          0.          0.        ]\n",
      "  [ 0.          0.          0.        ]\n",
      "  [ 0.          0.          0.        ]]]\n",
      "可以看到所有未登录词的词向量是一样的。\n"
     ]
    }
   ],
   "source": [
    "test_embedding_result = model.predict(test_id_array)\n",
    "print('测试集句子列表：')\n",
    "print(test_sent_lst)\n",
    "print('-'*100)\n",
    "print('测试集词向量列表：')\n",
    "print(test_embedding_result)\n",
    "print('可以看到所有未登录词的词向量是一样的。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TensorFlow中如何使用Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子列表：\n",
      "['a b c d e', 'a d e', 'c b']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "词向量列表：\n",
      "[[[ 0.418     0.24968  -0.41242 ]\n",
      "  [ 0.013441  0.23682  -0.16899 ]\n",
      "  [ 0.70853   0.57088  -0.4716  ]\n",
      "  [ 0.68047  -0.039263  0.30186 ]\n",
      "  [ 0.26818   0.14346  -0.27877 ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]]\n",
      "\n",
      " [[ 0.418     0.24968  -0.41242 ]\n",
      "  [ 0.68047  -0.039263  0.30186 ]\n",
      "  [ 0.26818   0.14346  -0.27877 ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]]\n",
      "\n",
      " [[ 0.70853   0.57088  -0.4716  ]\n",
      "  [ 0.013441  0.23682  -0.16899 ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "测试集句子列表：\n",
      "['a c e bbb a as', 'a b de ab df as d bb c ad', 'ab bc cd de']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "测试集词向量列表：\n",
      "[[[ 0.418       0.24968    -0.41242   ]\n",
      "  [ 0.70853     0.57088    -0.4716    ]\n",
      "  [ 0.26818     0.14346    -0.27877   ]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.418       0.24968    -0.41242   ]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.418       0.24968    -0.41242   ]\n",
      "  [ 0.013441    0.23682    -0.16899   ]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.68047    -0.039263    0.30186   ]]\n",
      "\n",
      " [[ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.09229596  0.06816354 -0.01181613]\n",
      "  [ 0.          0.          0.        ]\n",
      "  [ 0.          0.          0.        ]\n",
      "  [ 0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "seq_input = tf.placeholder(tf.int32, shape=(None, None))  # sentence * seq_len\n",
    "embedding_param = tf.Variable(initial_value=preprocessor.weights, dtype=tf.float32)  # 定义embedding为变量，给他初始化\n",
    "embedding_layer = tf.nn.embedding_lookup(embedding_param, seq_input)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('句子列表：')\n",
    "    print(sent_lst)\n",
    "    print('-'*100)\n",
    "    print('词向量列表：')\n",
    "    print(sess.run(embedding_layer, feed_dict={seq_input: id_array}))\n",
    "    print('-'*100)\n",
    "    print('测试集句子列表：')\n",
    "    print(test_sent_lst)\n",
    "    print('-'*100)\n",
    "    print('测试集词向量列表：')\n",
    "    print(sess.run(embedding_layer, feed_dict={seq_input: test_id_array}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过程序，我们学习了如何用TextPreprocessor类快速fit训练集  \n",
    "* 首先 build_word_freq_dct 函数构建词频统计字典，告知总共有多少个词。  \n",
    "* 然后 build_word2id 函数构造词汇表和 word2id 字典。可以限制词表大小，按词频做截断。如不设置则保留所有词。\n",
    "* view_sent_length_freq 函数输出句子长度（分词之后的词数量）分布。\n",
    "* get_truncate_id_list 返回一个句子序列截断后的id序列，句子的原始长度序列。 可以fit测试集句子序列。\n",
    "* build_weights 构建词向量 matrix 有返回值，同时保存至 self.weights\n",
    "### **序号0留给padding，1留给未登录词，2-5留给4个预设值占位符，所以词表的第一个词序号是6。keras的Embedding层需要输入字典长度，直接用len(weights)就好了，已经把padding用的0算在里头了。**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
